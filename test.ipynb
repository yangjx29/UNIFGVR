{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f23ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yjx/cache/conda/envs/finer/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/yjx/cache/conda/envs/finer/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/data/yjx/cache/conda/envs/finer/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00036106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yjx/cache/conda/envs/finer/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/yjx/cache/conda/envs/finer/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/data/yjx/cache/conda/envs/finer/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.43s/it]\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_shape:[13, 18]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('TkAgg')  # 设置交互式后端以显示图片\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.util import *\n",
    "from agents.qwen2_5_methods import rel_attention_qwen2_5\n",
    "\n",
    "model_name = 'qwen2_5'\n",
    "method_name = 'rel_att'\n",
    "image_path = '/data/yjx/MLLM/UniFGVR/datasets/dogs_120/images_discovery_all_1/032.Kerry_Blue_Terrier/032.Kerry_Blue_Terrier_n02093859_393.jpg'\n",
    "question = \"Describe the most significant features that can identify the type of dog in the image, and ultimately tell me what kind of dog it is.\"\n",
    "short_question = \"Describe the most significant features that can identify the type of dog in the image, and ultimately tell me what kind of dog it is.\"\n",
    "device = 'cuda'\n",
    "model_path = '/home/Dataset/Models/Qwen/Qwen2.5-VL-7B-Instruct'\n",
    "\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=\"auto\",\n",
    "                attn_implementation=\"eager\",\n",
    "                torch_dtype=torch.float32,\n",
    "            ).eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_path,\n",
    "    # trust_remote_code=True,\n",
    "    # padding_side='left',\n",
    "    # use_fast=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Visualize the bounding box\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# question = \"What is the pose of the woman with yellow backpack?\\n(A) walking\\n(B) running\\n(C) squatting\\n(D) standing\\nAnswer with the option's letter from the given choices directly.\"\n",
    "# short_question = \"What is the pose of the woman with yellow backpack?\\n(A) walking\\n(B) running\\n(C) squatting\\n(D) standing\\nAnswer with the option's letter from the given choices directly.\"\n",
    "question = \"Describe the most significant features that can identify the type of dog in the image, and ultimately tell me what kind of dog it is.\"\n",
    "short_question = \"Describe the most significant features that can identify the type of dog in the image, and ultimately tell me what kind of dog it is.\"\n",
    "general_question = 'Write a general description of the image.'\n",
    "prompt = f\"<image>\\nUSER: {question} Answer the question using a single word or phrase.\\nASSISTANT:\"\n",
    "general_prompt = f\"<image>\\nUSER: {general_question} Answer the question using a single word or phrase.\\nASSISTANT:\"\n",
    "# image_path = 'images/demo3.png'\n",
    "# image = Image.open(image_path).convert(\"RGB\")\n",
    "# att_map = high_res(rel_attention_qwen2_5, image, prompt, general_prompt, model, processor)\n",
    "att_map = rel_attention_qwen2_5(image, prompt, general_prompt, model, processor)\n",
    "# important_tokens_info = get_important_image_tokens(att_map, inputs, model, threshold=1)\n",
    "# print(f\"图像token位置范围: {important_tokens_info['positions']}\")\n",
    "# print(f\"重要stoken数量: {len(important_tokens_info['important_indices'])}\")\n",
    "# print(f\"最高注意力权重: {important_tokens_info['important_weights'].max().item():.4f}\")\n",
    "# print(f\"平均注意力权重: {important_tokens_info['important_weights'].mean().item():.4f}\")\n",
    "# print(f'att_map:{att_map}')\n",
    "# print(f'att_map shape:{att_map.shape}')\n",
    "plt.imshow(att_map, interpolation='none')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
